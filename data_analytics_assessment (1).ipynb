{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2-3wKjen1fl"
   },
   "source": [
    "Imports (pandas, numpy, seaborn/matplotlib, scikit-learn models & metrics)\n",
    "\n",
    "Intuition: Set up a standard analytics + ML toolkit for tabular classification and diagnostic plots.\n",
    "\n",
    "Justification: pandas/numpy for data handling; seaborn/matplotlib for quick EDA; scikit-learn for pipelines, scaling, model zoo (LR, SVM, trees), and evaluation (ROC, confusion matrix) all in one API.\n",
    "\n",
    "Interpretation: Nothing to interpret yet—this establishes capabilities we’ll rely on later (scaling for linear/maximum-margin models; tree ensembles that don’t need scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b7IfkMN0Y5oG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "esjTWPi1qb_o",
    "outputId": "148216fa-1ad1-4470-954b-f4a9417462fc"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\assessment_deploy_stremlit\\\\Students Performance Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124massessment_deploy_stremlit\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mStudents Performance Dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\assessment_deploy_stremlit\\\\Students Performance Dataset.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('F:\\\\assessment_deploy_stremlit\\\\Students Performance Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn5jLfIjoFH7"
   },
   "source": [
    "Countplot of Grade\n",
    "\n",
    "Intuition: Check class balance across letter grades to understand performance distribution and potential label engineering opportunities.\n",
    "\n",
    "Justification: A simple countplot surfaces skew and rare categories; deprecation warnings are about seaborn API but don’t affect insights.\n",
    "\n",
    "Interpretation: The relative volume of lower grades supports a binary risk construction (e.g., D/F ⇒ high risk) used in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "aDnoFSHhOJgn",
    "outputId": "90e842b2-c68c-428f-d3f0-e5d3227062f8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='Grade', palette='viridis')\n",
    "\n",
    "plt.title('Count of Each Category in Drop Out Risk')\n",
    "plt.xlabel('Drop Out Risk')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsT1QJgPoQuq"
   },
   "source": [
    "Create binary target Dropout_Risk from Grade and plot\n",
    "\n",
    "Intuition: Operationalize “at-risk” students by mapping low grades (D/F) to 1; everyone else to 0. Then check class balance.\n",
    "\n",
    "Justification: Binary framing simplifies early modeling and aligns with many institutional interventions (“at-risk” vs “not”). Plot confirms how imbalanced the label might be a key factor for metric choice and class weights.\n",
    "\n",
    "Interpretation: The subsequent statistics show the positive class rate ~40.8%, i.e., moderately imbalanced but not extreme; this motivates using class weights and recall/AUC, not just accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5C0AKsLrovF"
   },
   "outputs": [],
   "source": [
    "df['Dropout_Risk'] = df['Grade'].apply(lambda x: 1 if x in ['D', 'F'] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "EZHDHCL_Esp5",
    "outputId": "bfd48f1b-67f4-4754-a236-cdb55bc4f0f3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='Dropout_Risk', palette='viridis')\n",
    "\n",
    "plt.title('Count of Each Category in Drop Out Risk')\n",
    "plt.xlabel('Drop Out Risk')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o9trRGjn5tt"
   },
   "source": [
    "Load & peek at data (read_csv, head)\n",
    "\n",
    "Intuition: Bring the student performance dataset into memory and preview a few rows to sanity-check schema.\n",
    "\n",
    "Justification: A quick head() confirms expected columns (IDs, demographics, scores, etc.) before we compute anything downstream.\n",
    "\n",
    "Interpretation: The table includes identifiers, categorical attributes (e.g., Gender, Department), continuous assessments, and an overall Grade—useful targets/priors for a dropout-risk label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "gHRhs9foz12Y",
    "outputId": "ce0f253b-05be-442b-d0fc-aa9342f6deca"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fnix3Kf3z4Sf",
    "outputId": "922c1283-f819-4142-9c9c-942d8d531470"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-O-V1Y1occY"
   },
   "source": [
    "Missing-value audit\n",
    "\n",
    "Intuition: Quantify missingness by column to plan imputation strategies.\n",
    "\n",
    "Justification: Only Parent_Education_Level is missing (1,025 rows) so targeted imputation avoids over-engineering.\n",
    "\n",
    "Interpretation: Proceed with numeric medians for a couple columns (defensive) and mode for this categorical; confirm no residual NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TslCTZBe4CqX",
    "outputId": "a0582028-2829-47ec-8336-9e23000d9497"
   },
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "print(\"Summary statistics of numeric columns:\\n\")\n",
    "basic_stats = df.describe()\n",
    "print(basic_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJw17gtR4OlT",
    "outputId": "5b32f337-e264-45dd-eb77-9e74d2e496e0"
   },
   "outputs": [],
   "source": [
    "df[\"Attendance (%)\"] = df[\"Attendance (%)\"].fillna(df[\"Attendance (%)\"].median())\n",
    "df[\"Assignments_Avg\"] = df[\"Assignments_Avg\"].fillna(df[\"Assignments_Avg\"].median())\n",
    "# Fill categorical column with mode\n",
    "df[\"Parent_Education_Level\"] = df[\"Parent_Education_Level\"].fillna(df[\"Parent_Education_Level\"].mode()[0])\n",
    "print(\"Remaining missing values:\\n\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WafS36eKohJL"
   },
   "source": [
    "One-hot encoding for categoricals (get_dummies)\n",
    "\n",
    "Intuition: Convert nominal features into ML-ready numeric indicators.\n",
    "\n",
    "Justification: Tree and linear models both benefit from dummy variables; scikit-learn expects numerics.\n",
    "Interpretation: Encoded frame reports 29 columns including boolean dummies for Gender/Department/Activities/Internet/Parent Education/Income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoGuNr1d0GQe",
    "outputId": "a5551d82-885f-4700-f37b-df2c92c67bea"
   },
   "outputs": [],
   "source": [
    "# Example: encode categorical columns\n",
    "df_encoded = pd.get_dummies(df, columns=['Gender', 'Department','Extracurricular_Activities','Internet_Access_at_Home','Parent_Education_Level','Family_Income_Level'], drop_first=True)\n",
    "\n",
    "# drop_first=True avoids dummy variable trap\n",
    "df_encoded.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "5i4RD8Hi4pET",
    "outputId": "d9c87453-6e09-488f-a3b8-f9bee75d9ddb"
   },
   "outputs": [],
   "source": [
    "with plt.style.context('ggplot'):\n",
    "  plt.figure(figsize=(8,5))\n",
    "  sns.histplot(df_encoded['Participation_Score'], bins=20, kde=True, color='skyblue')\n",
    "  plt.title('Distribution of Participation Score')\n",
    "  plt.xlabel('Participation Score')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "Xxo8-q4VARZo",
    "outputId": "75309ab0-27fd-4a69-b6fa-1f28c77fad18"
   },
   "outputs": [],
   "source": [
    "with plt.style.context('ggplot'):\n",
    "  plt.figure(figsize=(8,5))\n",
    "  sns.histplot(df_encoded['Projects_Score'], bins=20, kde=True, color='skyblue')\n",
    "  plt.title('Distribution of Project Score')\n",
    "  plt.xlabel('Project Score')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "mBevP7nKAmFc",
    "outputId": "1c4a488c-c422-4d93-9dd3-81648999530c"
   },
   "outputs": [],
   "source": [
    "with plt.style.context('ggplot'):\n",
    "  plt.figure(figsize=(8,5))\n",
    "  sns.histplot(df_encoded['Study_Hours_per_Week'], bins=20, kde=True, color='skyblue')\n",
    "  plt.title('Distribution of Study Hours Per Week')\n",
    "  plt.xlabel('Study Hours')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsYZejapopMh"
   },
   "source": [
    "Correlation heatmap & targeted correlations with Dropout_Risk\n",
    "\n",
    "Intuition: Identify features most linearly associated with the target to guide feature salience and set expectations.\n",
    "\n",
    "\n",
    "Justification: Correlations are not causation, but they’re fast signal checks to see which variables might dominate simple models.\n",
    "Interpretation: Strong negative correlations for Total_Score (-0.80), Final_Score (-0.48), Projects_Score (-0.47) with risk—consistent with academic performance being protective. Expect these to drive tree splits and linear coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "eaE-CIMRBsoc",
    "outputId": "25b5bd50-f7d4-44d3-8613-88210209b5a8"
   },
   "outputs": [],
   "source": [
    "# 1. Compute correlation matrix\n",
    "corr_matrix = df.corr()  # numeric_only avoids non-numeric errors\n",
    "\n",
    "# 2. Plot colorful correlation heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 3. Get correlation values with Dropout_Risk column\n",
    "target_corr = corr_matrix['Dropout_Risk'].sort_values(ascending=False)\n",
    "\n",
    "# 4. Filter variables with strong correlation\n",
    "strong_corr = target_corr[(target_corr > 0.3) | (target_corr < -0.3)]\n",
    "\n",
    "print(\"Variables strongly correlated with Dropout_Risk:\")\n",
    "print(strong_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwXgb96NURiW",
    "outputId": "c85f533c-3ab3-41ba-9800-db94a69f851e"
   },
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbWcmTMNovDw"
   },
   "source": [
    "Drop ID/leaky columns and define X, y\n",
    "\n",
    "Intuition: Remove identifiers and Grade (used to derive the target) to prevent leakage; set up features/labels.\n",
    "\n",
    "Justification: IDs add noise; Grade is too proximate to Dropout_Risk (derived), so keeping it would inflate performance unrealistically.\n",
    "Interpretation: y is the binary risk; X contains clean numeric + dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LAQAVgKSJz7"
   },
   "outputs": [],
   "source": [
    "df_encoded.drop(['Student_ID','First_Name','Last_Name','Email','Grade'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8fTAZkmVGrr"
   },
   "outputs": [],
   "source": [
    "y=df_encoded['Dropout_Risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2HkPa6JVKau"
   },
   "outputs": [],
   "source": [
    "x=df_encoded.drop('Dropout_Risk',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEgR2Gj0o1IU"
   },
   "source": [
    "Modeling pipelines (LR, SVM, Decision Tree, Random Forest)\n",
    "\n",
    "Intuition: Compare linear, maximum-margin, and tree-based learners to balance interpretability and non-linear capacity. Include scaling where needed.\n",
    "\n",
    "Justification:\n",
    "\n",
    "Standardize for LR/SVM; trees don’t need it.\n",
    "\n",
    "Use class_weight=\"balanced\" (or subsample) to reflect ~60/40 class split and to protect recall for the minority class.\n",
    "\n",
    "Interpretation: Expect Random Forest to capture non-linear interactions (e.g., hours × scores), SVM/LR to provide clean decision boundaries, and trees to be interpretable but variance-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A7pjpO54TST_",
    "outputId": "ecac7690-9da4-4920-b3a9-100b93440f1d"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ===== Pipelines (scale for LR/SVM; trees don’t need scaling)\n",
    "pipelines = {\n",
    "    \"LogisticRegression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42))\n",
    "    ]),\n",
    "    \"SVM\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(probability=True, class_weight=\"balanced\", random_state=42))\n",
    "    ]),\n",
    "    \"DecisionTree\": Pipeline([\n",
    "        (\"clf\", DecisionTreeClassifier(class_weight=\"balanced\", random_state=42))\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=300, class_weight=\"balanced_subsample\", random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# ===== Fit, evaluate, collect metrics\n",
    "metrics_rows = []\n",
    "roc_curves = {}  # model -> (fpr, tpr, auc)\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    # Probabilities for ROC\n",
    "    if hasattr(pipe.named_steps[list(pipe.named_steps.keys())[-1]], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # fallback for models without predict_proba (not needed here since SVC has probability=True)\n",
    "        # using decision_function, then min-max to [0,1]\n",
    "        raw = pipe.decision_function(X_test)\n",
    "        y_proba = (raw - raw.min()) / (raw.max() - raw.min() + 1e-12)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    metrics_rows.append([name, acc, prec, rec, f1, auc])\n",
    "\n",
    "    # ROC curve points\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_curves[name] = (fpr, tpr, auc)\n",
    "\n",
    "# ===== Metrics table\n",
    "metrics_df = pd.DataFrame(\n",
    "    metrics_rows,\n",
    "    columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC_AUC\"]\n",
    ").sort_values(by=\"ROC_AUC\", ascending=False)\n",
    "print(\"=== Model Performance ===\")\n",
    "print(metrics_df)\n",
    "\n",
    "best_by_auc = metrics_df.iloc[0][\"Model\"]\n",
    "best_by_recall = metrics_df.sort_values(by=\"Recall\", ascending=False).iloc[0][\"Model\"]\n",
    "print(f\"\\nBest by ROC_AUC: {best_by_auc}\")\n",
    "print(f\"Best by Recall:  {best_by_recall}\")\n",
    "\n",
    "# ===== Plot ROC curves (single chart). NOTE: no custom colors per your plotting rules.\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, (fpr, tpr, auc) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\")\n",
    "plt.show()\n",
    "\n",
    "# ===== Confusion matrices for each model\n",
    "def plot_confusion(cm, title):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    # Write counts on cells\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion(cm, f\"Confusion Matrix — {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toJT40qOo_st"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Interpretation of Model Performance**\n",
    "\n",
    "### **Overall Evaluation Approach**\n",
    "\n",
    "The notebook compared four classifiers—**Logistic Regression**, **SVM**, **Decision Tree**, and **Random Forest**on a binary dropout-risk label derived from grades.\n",
    "Each was evaluated on **Accuracy, Precision, Recall, F1-score, and ROC-AUC**, with **ROC curves** and **confusion matrices** providing additional diagnostic insight.\n",
    "Class imbalance (\\~41% “at-risk”) was moderate but sufficient to warrant attention to **Recall** and **AUC**, rather than Accuracy alone.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Performance Insights**\n",
    "\n",
    "1. **ROC-AUC as Primary Ranking Metric**\n",
    "\n",
    "   * The ROC-AUC values showed a clear separation between models.\n",
    "   * **Random Forest** achieved the **highest AUC**, indicating it was best at ranking at-risk vs. safe students across all possible thresholds.\n",
    "   * This suggests that non-linear feature interactions (e.g., project score × study hours) are important for predicting dropout risk something tree ensembles capture well.\n",
    "\n",
    "2. **Recall (Sensitivity) for Catching At-Risk Students**\n",
    "\n",
    "   * If the priority is *not missing* at-risk students (low false negatives), **Random Forest** and **Decision Tree** delivered higher recall than SVM and Logistic Regression.\n",
    "   * **Logistic Regression** had lower recall, meaning it would fail to identify a larger fraction of true at-risk cases—risky for intervention purposes.\n",
    "\n",
    "3. **Precision and False Alarms**\n",
    "\n",
    "   * Logistic Regression and SVM tended to have **higher precision** but lower recall. This means fewer false positives, but at the cost of missing real at-risk students.\n",
    "   * In a student support context, this is often an undesirable trade-off because missing a high-risk student is usually more costly than falsely flagging a low-risk one.\n",
    "\n",
    "4. **F1-Score (Balance Between Precision & Recall)**\n",
    "\n",
    "   * **Random Forest** showed strong F1-scores, balancing high recall with acceptable precision.\n",
    "   * This reinforces it as the most balanced choice if we want both good catch rates and manageable intervention loads.\n",
    "\n",
    "5. **Confusion Matrices**\n",
    "\n",
    "   * For Random Forest, the **true positive count was the highest** among models, meaning it correctly identified more at-risk students.\n",
    "   * The trade-off: slightly higher false positives compared to more conservative models.\n",
    "   * For Decision Tree, a similar pattern emerged, but with more overfitting risk (likely why its ROC-AUC was lower than Random Forest).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Implications**\n",
    "\n",
    "* **Random Forest** is the most reliable for ranking and catching at-risk students and should be the first choice if the goal is proactive intervention.\n",
    "* For operational use, you should **tune the decision threshold** for Random Forest to push recall even higher if resources allow for more follow-up on false positives.\n",
    "* Logistic Regression or SVM could be used in contexts where false positives are very costly (e.g., if interventions are expensive or intrusive), but only if we accept missing more real cases.\n",
    "* Decision Tree could be used for explainability, but Random Forest offers both performance and some interpretability (via feature importance).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# X_train, y_train are your prepared features and target\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"random_forest_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
